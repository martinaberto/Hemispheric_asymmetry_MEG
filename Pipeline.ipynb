{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from mne.stats import spatio_temporal_cluster_1samp_test\n",
    "from scipy import stats\n",
    "from mne import (pick_events, find_events, read_source_spaces, spatial_src_adjacency, \n",
    "                 make_forward_solution, compute_rank, compute_raw_covariance, pick_info)\n",
    "from mne.io import read_info\n",
    "from mne import Epochs\n",
    "from mne.minimum_norm import make_inverse_operator\n",
    "from mne.preprocessing import ICA\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import glob\n",
    "from mne.minimum_norm import apply_inverse\n",
    "from mne.epochs import equalize_epoch_counts\n",
    "from mne.preprocessing import corrmap\n",
    "import utils.util_functions as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING\n",
    "### Compute ICA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ica(subject_id, raw_dir, ica_dir):\n",
    "\n",
    "    joblib.load(join(raw_dir, subject_id))\n",
    "    # compute ica on concatenated data (all 6 blocks)\n",
    "    raw_copy = subject_id.copy().filter(l_freq=1, h_freq=None)\n",
    "    raw_filt = raw_copy.resample(sfreq=250)\n",
    "    raw_filt.set_annotations(None)\n",
    "    \n",
    "    ica = ICA(n_components=None, max_iter='auto', random_state=97)\n",
    "    ica.fit(raw_filt)\n",
    "    #plot \n",
    "    fig= ica.plot_components(picks=range(0,20))\n",
    "    fig.savefig(join(subject_id +'.png'))\n",
    "    \n",
    "    #save ica\n",
    "    joblib.dump(ica, join(ica_dir, subject_id))    \n",
    "        \n",
    "    return ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CORRMAP to identify and remove bad components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir= 'raw_dir' # path with concatenated raw data\n",
    "ica_dir= 'ica_dir' # path with ica weights\n",
    "\n",
    "all_subjects= listdir(raw_dir)\n",
    "\n",
    "icas=[]\n",
    "for sub in all_subjects:\n",
    "    # load ica weights\n",
    "    ica= joblib.load(join(ica_dir, sub))\n",
    "    icas.append(ica)\n",
    "  \n",
    "del ica\n",
    "    \n",
    "#  I choose subject 19860620hrwc as template for eyemov \n",
    "subject_id='19860620hrwc'\n",
    "\n",
    "# load raw file and downsample it \n",
    "raw= joblib.load(join(raw_dir, subject_id))\n",
    "#raw = raw.resample(sfreq=250)\n",
    "\n",
    "template= all_subjects.index(subject_id)\n",
    "raw.load_data()\n",
    "icas[template].plot_sources(raw)\n",
    "\n",
    "# EYES\n",
    "eog_inds, eog_scores = icas[template].find_bads_eog(raw, reject_by_annotation=False)\n",
    "corrmap(icas, template=(template, eog_inds[0]), ch_type='mag', plot=False)\n",
    "\n",
    "# check for the correct threshold which include all of the partecipant\n",
    "threshold=0.85 # reduce the value untill you have \"At least 1 IC detected for each subject.\" as output\n",
    "corrmap(icas, template=(template, eog_inds[0]), ch_type='mag', threshold=threshold, plot=False)\n",
    "\n",
    "# now mark them in label as eyemovements\n",
    "corrmap(icas, template=(template, eog_inds[0]), ch_type='mag', threshold=threshold, label='eyemov')\n",
    "\n",
    "# plot topography for each marked components and if you have doubts load raw file and visually inspect\n",
    "\n",
    "for ii, ica in enumerate(icas):\n",
    "  icas[ii].plot_components(picks=icas[ii].labels_['eyemov'])\n",
    "\n",
    "# now do the same for heartbeat\n",
    "# HEARTBEAT\n",
    "# I choose subject 8: 19860620hrwc as template for eyemov\n",
    "subject_id='19921030jard'\n",
    "raw= joblib.load(join(raw_dir, subject_id))\n",
    "raw = raw.resample(sfreq=250)\n",
    "\n",
    "template= all_subjects.index(subject_id)\n",
    "raw.load_data()\n",
    "\n",
    "ecg_inds, ecg_scores = icas[template].find_bads_ecg(raw, reject_by_annotation=False)\n",
    "icas[template].plot_sources(raw)\n",
    "\n",
    "corrmap(icas, template=(template, ecg_inds[0]), ch_type='mag', plot=False)\n",
    "\n",
    "threshold=0.6 # reduce the value untill you have \"At least 1 IC detected for each subject.\" as output\n",
    "corrmap(icas, template=(template, ecg_inds[0]), ch_type='mag', threshold=threshold, plot=False)\n",
    "\n",
    "corrmap(icas, template=(template, ecg_inds[0]), ch_type='mag', threshold=threshold, label='heart')\n",
    "\n",
    "for ii, ica in enumerate(icas):\n",
    "  ica.plot_components(picks=icas[ii].labels_['heart'])\n",
    "\n",
    "# add components to exclude and check them\n",
    "for ii, ica in enumerate(icas):\n",
    "  icas[ii].exclude=[]\n",
    "  icas[ii].exclude = icas[ii].labels_['eyemov']\n",
    "  icas[ii].exclude.extend(icas[ii].labels_['heart'])\n",
    "  \n",
    "dict_icas= {}\n",
    "for ii, sub in enumerate(all_subjects):\n",
    "  \n",
    "  dict_icas.update({sub: icas[ii]})\n",
    "\n",
    "joblib.dump(dict_icas, join(ica_dir, 'all_icas')) \n",
    "\n",
    "\n",
    "# Get info on removed components to report\n",
    "badcomps=[]\n",
    "for sub in all_subjects:\n",
    "  \n",
    "  badcomps.append(len(icas[sub].exclude))\n",
    "\n",
    "avg_num_icas= round(np.mean(badcomps), 2)\n",
    "std_num_icas= round(np.std(badcomps), 2)\n",
    "range_num_inca= [np.min(badcomps), np.max(badcomps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply ICA weigths on raw unfiltered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ica(subject_id, raw_dir, ica_dir, out_dir):\n",
    "        \n",
    "        # load concatenated raw data     \n",
    "        raw= joblib.load(join(raw_dir, subject_id))\n",
    "        \n",
    "        # load ica weigths\n",
    "        icas= joblib.load(join(ica_dir, 'all_icas'))\n",
    "        \n",
    "        # apply ica weights\n",
    "        ica= icas[subject_id]\n",
    "        ica.apply(raw)\n",
    "        \n",
    "        #save preprocessed data\n",
    "        joblib.dump(raw, join(out_dir, subject_id))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOURCE SPACE ANALYSIS\n",
    "### Compute forward and inverse solution\n",
    "\n",
    "Note: Before using the following function, use coregistration script in utils to create headmodels starting from fsaverage template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_trans(subject_id, raw_path, fs_path, ftrans, save_path):\n",
    "    \n",
    "        # define src and bem files\n",
    "        src = join(fs_path, 'fsaverage-ico-4-src.fif')\n",
    "        bem = join(fs_path, 'fsaverage-5120-5120-5120-bem-sol.fif')\n",
    "        \n",
    "        # use individual coregistrations   \n",
    "        trans =  join(ftrans, subject_id + '-trans.fif')  \n",
    "        \n",
    "        fname = glob.glob(join(raw_path, '**/%s_block1_trans_sss.fif' % subject_id),\n",
    "                            recursive=True)\n",
    "\n",
    "        # get info for MNE from a random dataset\n",
    "        info = read_info(fname[0], verbose=False)\n",
    "        info = pick_info(info, pick_types(info, meg=True))\n",
    "        \n",
    "        # compute noise cov on empty room\n",
    "        empty_room = ut.get_nearest_empty_room(info)\n",
    "\n",
    "        empty_room = ut.preproc_data(empty_room, max_filt=True, coord_frame='meg', notch=False,\n",
    "                                  apply_filter=True, hp_lp_freqs=(0.1, 40.), do_downsample=True,\n",
    "                                  resample_freq=250.)\n",
    "\n",
    "        noise_cov = compute_raw_covariance(empty_room, rank=None, picks='meg')\n",
    "       \n",
    "        true_rank = compute_rank(noise_cov, info=empty_room.info)  # inferring true rank\n",
    "\n",
    "        # make forward solution for MNE\n",
    "        fwd = make_forward_solution(info=info, trans=trans, src=src, bem=bem, meg=True, eeg=False)\n",
    "\n",
    "        # compute inverse operator for MNE\n",
    "        inv = minimum_norm.make_inverse_operator(info, fwd, noise_cov, rank=true_rank)  # rank=None\n",
    "\n",
    "        data2save = {'inv': inv, 'fwd': fwd, 'noise_cov': noise_cov, \n",
    "                     'empty_room_info': empty_room.info}\n",
    "\n",
    "        joblib.dump(data2save, join(save_path, subject_id + '.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoching, computing ERFs, and apply inverse solution to compute source timecourses (stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoching_source(data, tmin, tmax, baseline, decim):\n",
    "    starting_trigger_list= [48, 80, 88, 104]\n",
    "    conditions= ['L40', 'L478', 'S40', 'S478']\n",
    "    events_all= find_events(data)\n",
    "    \n",
    "    epoched_all={}\n",
    "    # first: epoching according to condition\n",
    "    for c, idx in enumerate(starting_trigger_list):\n",
    "      pos= np.where((events_all==idx))[0]\n",
    "      start= ((events_all[pos,0]-100)/1000)-(data.first_samp/data.info['sfreq'])\n",
    "      ending= start+327\n",
    "      first_epoch= data.copy().crop(tmin= start[0], tmax= ending[0], include_tmax= True)\n",
    "      events= mne.find_events(first_epoch)\n",
    "        \n",
    "      # second: create bsl corrected epochs for erps (epoch novel)\n",
    "      ev= pick_events(events, include= 64)\n",
    "      event_id= {'novel': 64}        \n",
    "      picks = pick_types(first_epoch.info, meg=True)    \n",
    "      \n",
    "      if decim== None:\n",
    "        decim= 1 \n",
    "\n",
    "      epochs= mne.Epochs(first_epoch, ev, event_id, tmin= tmin, tmax= tmax, \n",
    "                   baseline= baseline, decim= decim, picks=picks, reject=None, \n",
    "                   reject_by_annotation=True, preload=True)\n",
    "        \n",
    "      epoched_all.update({conditions[c]: epochs})\n",
    "    \n",
    "    return epoched_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stcs(cond, subject_id, data_path, out_dir, src_path):\n",
    "        \n",
    "    if cond== 40:\n",
    "        c= ['L40', 'S40']\n",
    "    elif cond == 478:\n",
    "        c= ['L478', 'S478']\n",
    "\n",
    "    # parameters for inverse solution\n",
    "    snr = 3.0\n",
    "    lambda2 = 1.0 / snr ** 2\n",
    "    method = \"MNE\" \n",
    "        \n",
    "    # Load source space for this subject \n",
    "    src= joblib.load(join(src_path, subject_id + '.dat'))\n",
    "    inverse_operator= src['inv']\n",
    "        \n",
    "    # Load epochs for novel sounds\n",
    "\n",
    "    epochs_nov= joblib.load(join(data_path, 'nov_' + subject_id))\n",
    "        \n",
    "    # select the condition and cut pre-stimulus period\n",
    "        \n",
    "    epochs1= epochs_nov[c[0]]\n",
    "    epochs2= epochs_nov[c[1]]\n",
    "    equalize_epoch_counts([epochs1, epochs2])  \n",
    "        \n",
    "    # compute evoked response\n",
    "    evoked1 = epochs1.average()\n",
    "    evoked2 = epochs2.average()\n",
    "        \n",
    "    # cropping t < 0 \n",
    "    evoked1.crop(0, None)\n",
    "    evoked2.crop(0, None)\n",
    "                         \n",
    "    # Apply inverse operator  \n",
    "    stcs1 = apply_inverse(evoked1, inverse_operator, lambda2, method)\n",
    "    stcs2 = apply_inverse(evoked2, inverse_operator, lambda2, method)\n",
    "        \n",
    "    joblib.dump(stcs1, join(out_dir, 'stcs' + '_' +c[0] + '_' + subject_id))  \n",
    "    joblib.dump(stcs2, join(out_dir, 'stcs' + '_' +c[1] + '_' + subject_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS\n",
    "### Make EELBRAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eelbrain as eb\n",
    "\n",
    "stc_dir= 'SOURCE/stcs'\n",
    "mri_avg= '/freesurfer/'\n",
    "data_path= join('SOURCE/epoched_novel/')\n",
    "\n",
    "stcs_list= [ x for x in sorted(listdir(stc_dir)) if \"stcs\" in x ]\n",
    "all_subjects = sorted(listdir(data_path))   \n",
    "\n",
    "\n",
    "data4ds = []\n",
    "for condition in stcs_list:\n",
    "\n",
    "        stcs= joblib.load(join(stc_dir, condition))\n",
    "    \n",
    "        idx_cond= condition.replace('stcs_', '')\n",
    "    \n",
    "        for ii, sub in enumerate(stcs):\n",
    "\n",
    "            stc= eb.load.fiff.stc_ndvar(sub, src= 'ico-4', subject='fsaverage', subjects_dir=mri_avg,\n",
    "                       method= 'MNE', name= 'stc')\n",
    "    \n",
    "            subject_id= all_subjects[ii]\n",
    "            \n",
    "            data4ds.append([subject_id, idx_cond, stc])\n",
    "\n",
    "ds = eb.Dataset.from_caselist(['subject_id', 'condition', 'stc'],\n",
    "                                  data4ds, random='subject_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform clusterbased permutation\n",
    "duration= '40' # or 478\n",
    "ds_plot= ds.sub(((ds[\"condition\"]== \"S\" + duration) \n",
    "                         | (ds[\"condition\"]== \"L\" + duration)))\n",
    "\n",
    "ttest = eb.testnd.TTestRelated(\"stc\", \"condition\", match=\"subject_id\", \n",
    "    ds=ds_plot, tail= 0,  tfce=True,  tstart=0, tstop=0.499)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
